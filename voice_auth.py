"""
Sistema de autenticaciÃ³n de voz con CHALLENGE-RESPONSE
Genera frases aleatorias en cada verificaciÃ³n para prevenir ataques de replay
"""

import numpy as np
import sounddevice as sd
from scipy.signal import butter, filtfilt
from scipy.spatial.distance import euclidean
from fastdtw import fastdtw
import librosa
from config import Config
from challenge_generator import ChallengeGenerator


class VoiceAuthChallenge:
    """
    Sistema de autenticaciÃ³n de voz con desafÃ­os aleatorios
    Previene ataques de replay al requerir diferentes frases cada vez
    """
    
    def __init__(self):
        self.sample_rate = Config.VOICE_SAMPLE_RATE
        self.duration = Config.VOICE_DURATION
        self.similarity_threshold = getattr(Config, 'VOICE_SIMILARITY_THRESHOLD', 0.75)

        # Tipo de desafÃ­o por defecto
        self.challenge_type = getattr(Config, 'VOICE_CHALLENGE_TYPE', 'numeric')

        # ParÃ¡metros MFCC
        self.n_mfcc = 13
        self.n_fft = 2048
        self.hop_length = 512

        # ParÃ¡metros de detecciÃ³n de vivacidad
        self.enable_liveness = getattr(Config, 'VOICE_ENABLE_LIVENESS', True)
        self.energy_variance_threshold = getattr(Config, 'VOICE_MIN_ENERGY_VARIANCE', 0.005)
        self.zcr_variance_threshold = getattr(Config, 'VOICE_MIN_ZCR_VARIANCE', 0.0005)
        self.pitch_variance_threshold = getattr(Config, 'VOICE_MIN_PITCH_VARIANCE', 2)

    def generate_challenge(self):
        """
        Genera una frase de desafÃ­o aleatoria
        Retorna solo el texto del desafÃ­o (para compatibilidad con Flask)
        """
        challenge_text, display_format = ChallengeGenerator.generate_challenge(self.challenge_type)
        return challenge_text
    
    def _apply_bandpass_filter(self, audio, lowcut=300, highcut=3400):
        """Aplica filtro pasabanda para voz humana"""
        nyquist = self.sample_rate / 2
        low = lowcut / nyquist
        high = highcut / nyquist
        b, a = butter(4, [low, high], btype='band')
        filtered = filtfilt(b, a, audio)
        return filtered
    
    def _normalize_audio(self, audio):
        """Normaliza el audio al rango [-1, 1]"""
        audio = audio.astype(np.float32)
        max_val = np.max(np.abs(audio))
        if max_val > 0:
            audio = audio / max_val
        return audio
    
    def _remove_silence(self, audio, threshold=0.02):
        """Elimina silencios del inicio y final"""
        window_size = int(self.sample_rate * 0.02)
        energy = np.array([
            np.sum(audio[i:i+window_size]**2) 
            for i in range(0, len(audio) - window_size, window_size)
        ])
        
        if np.max(energy) > 0:
            energy = energy / np.max(energy)
        
        voice_indices = np.where(energy > threshold)[0]
        
        if len(voice_indices) == 0:
            return audio
        
        start_idx = voice_indices[0] * window_size
        end_idx = (voice_indices[-1] + 1) * window_size
        
        return audio[start_idx:end_idx]
    
    def _extract_mfcc_features(self, audio):
        """Extrae caracterÃ­sticas MFCC + deltas y calcula estadÃ­sticas"""
        mfcc = librosa.feature.mfcc(
            y=audio,
            sr=self.sample_rate,
            n_mfcc=self.n_mfcc,
            n_fft=self.n_fft,
            hop_length=self.hop_length
        )

        mfcc_delta = librosa.feature.delta(mfcc)
        mfcc_delta2 = librosa.feature.delta(mfcc, order=2)

        features = np.vstack([mfcc, mfcc_delta, mfcc_delta2])

        return features

    def _extract_speaker_embedding(self, mfcc_features):
        """
        Extrae un vector de embedding del hablante independiente del texto
        Calcula estadÃ­sticas sobre los MFCC que caracterizan la voz, no el contenido
        """
        # Calcular estadÃ­sticas de primer y segundo orden
        mean = np.mean(mfcc_features, axis=1)
        std = np.std(mfcc_features, axis=1)

        # Percentiles para capturar la distribuciÃ³n
        percentile_25 = np.percentile(mfcc_features, 25, axis=1)
        percentile_75 = np.percentile(mfcc_features, 75, axis=1)

        # Rango intercuartÃ­lico
        iqr = percentile_75 - percentile_25

        # Skewness y kurtosis aproximados
        median = np.median(mfcc_features, axis=1)
        skewness = mean - median

        # Concatenar todas las estadÃ­sticas
        embedding = np.concatenate([
            mean,           # 39 valores
            std,            # 39 valores
            percentile_25,  # 39 valores
            percentile_75,  # 39 valores
            iqr,            # 39 valores
            skewness        # 39 valores
        ])

        return embedding  # Vector de 234 dimensiones
    
    def _extract_prosodic_features(self, audio):
        """Extrae caracterÃ­sticas prosÃ³dicas"""
        rms = librosa.feature.rms(
            y=audio,
            frame_length=self.n_fft,
            hop_length=self.hop_length
        )[0]
        
        zcr = librosa.feature.zero_crossing_rate(
            audio,
            frame_length=self.n_fft,
            hop_length=self.hop_length
        )[0]
        
        try:
            pitches, magnitudes = librosa.piptrack(
                y=audio,
                sr=self.sample_rate,
                n_fft=self.n_fft,
                hop_length=self.hop_length
            )
            pitch = []
            for t in range(pitches.shape[1]):
                index = magnitudes[:, t].argmax()
                pitch.append(pitches[index, t])
            pitch = np.array(pitch)
        except:
            pitch = np.zeros(len(rms))
        
        return {
            'rms': rms,
            'zcr': zcr,
            'pitch': pitch,
            'rms_variance': np.var(rms),
            'zcr_variance': np.var(zcr),
            'pitch_variance': np.var(pitch[pitch > 0]) if np.any(pitch > 0) else 0
        }
    
    def _check_liveness(self, prosodic_features):
        """Verifica vivacidad del audio"""
        checks = []
        messages = []
        
        energy_var = prosodic_features['rms_variance']
        energy_check = energy_var > self.energy_variance_threshold
        checks.append(energy_check)
        
        if energy_check:
            messages.append(f"âœ“ EnergÃ­a natural ({energy_var:.6f})")
        else:
            messages.append(f"âš ï¸  EnergÃ­a baja ({energy_var:.6f})")
        
        zcr_var = prosodic_features['zcr_variance']
        zcr_check = zcr_var > self.zcr_variance_threshold
        checks.append(zcr_check)
        
        if zcr_check:
            messages.append(f"âœ“ ZCR natural ({zcr_var:.6f})")
        else:
            messages.append(f"âš ï¸  ZCR bajo ({zcr_var:.6f})")
        
        pitch_var = prosodic_features['pitch_variance']
        pitch_check = pitch_var > self.pitch_variance_threshold
        checks.append(pitch_check)
        
        if pitch_check:
            messages.append(f"âœ“ Pitch natural ({pitch_var:.2f})")
        else:
            messages.append(f"âš ï¸  Pitch bajo ({pitch_var:.2f})")
        
        confidence = sum(checks) / len(checks)
        is_live = confidence >= 0.34
        
        if sum(checks) == 0:
            is_live = False
            messages.append("âš ï¸  Audio puede ser sintÃ©tico")
        
        return is_live, confidence, messages
    
    def _compare_embeddings(self, embedding1, embedding2):
        """
        Compara embeddings de hablante usando similitud coseno
        MÃ¡s apropiado para vectores de caracterÃ­sticas estadÃ­sticas
        """
        try:
            emb1 = np.array(embedding1)
            emb2 = np.array(embedding2)

            # Normalizar vectores
            emb1_norm = emb1 / (np.linalg.norm(emb1) + 1e-8)
            emb2_norm = emb2 / (np.linalg.norm(emb2) + 1e-8)

            # Calcular similitud coseno
            cosine_similarity = np.dot(emb1_norm, emb2_norm)

            # Convertir de [-1, 1] a [0, 1]
            similarity = (cosine_similarity + 1) / 2

            return similarity

        except Exception as e:
            print(f"\n   âš ï¸  Error en comparaciÃ³n de embeddings: {e}")
            return 0.0

    def _compare_features_dtw(self, features1, features2):
        """Compara caracterÃ­sticas usando DTW"""
        try:
            feat1 = np.array(features1)
            feat2 = np.array(features2)

            if feat1.ndim == 1:
                feat1 = feat1.reshape(-1, 1)
            if feat2.ndim == 1:
                feat2 = feat2.reshape(-1, 1)

            # Transponer para que cada fila sea un frame temporal
            feat1 = feat1.T
            feat2 = feat2.T

            # Calcular DTW
            distance, path = fastdtw(feat1, feat2, dist=euclidean)

            # Normalizar por el nÃºmero de frames y dimensiones
            avg_length = (len(feat1) + len(feat2)) / 2
            n_features = feat1.shape[1]

            # NormalizaciÃ³n mejorada para MFCC
            normalized_distance = distance / (avg_length * np.sqrt(n_features))

            # Convertir a similitud (0-1)
            similarity = 1 / (1 + normalized_distance)

            return similarity, normalized_distance

        except Exception as e:
            print(f"\n   âš ï¸  Error en comparaciÃ³n DTW: {e}")
            return 0.0, float('inf')
    
    def _record_audio_with_challenge(self, challenge_text, display_format):
        """Graba audio mostrando el desafÃ­o al usuario"""
        print(f"\n{display_format}")
        print(f"\n   â•”{'â•'*56}â•—")
        print(f"   â•‘  {challenge_text:^52}  â•‘")
        print(f"   â•š{'â•'*56}â•")
        
        print(f"\nâ±ï¸  DuraciÃ³n: {self.duration} segundos")
        print("\nğŸ”´ Grabando en 3...")
        
        import time
        for i in range(3, 0, -1):
            print(f"   {i}...")
            time.sleep(1)
        
        print("ğŸ™ï¸  Â¡HABLA AHORA!\n")
        
        recording = sd.rec(
            int(self.duration * self.sample_rate),
            samplerate=self.sample_rate,
            channels=1,
            dtype=np.float32
        )
        sd.wait()
        
        print("âœ… GrabaciÃ³n completada\n")
        
        return recording.flatten()
    
    def _process_audio(self, audio):
        """Pipeline completo de procesamiento"""
        print("   ğŸ”„ Procesando audio...")

        audio = self._normalize_audio(audio)
        print("      âœ“ Normalizado")

        audio = self._apply_bandpass_filter(audio)
        print("      âœ“ Filtrado")

        audio = self._remove_silence(audio)
        print("      âœ“ Silencios eliminados")

        min_length = self.sample_rate * 1.0
        if len(audio) < min_length:
            print("      âš ï¸  Audio muy corto")
            return None, None, None

        mfcc_features = self._extract_mfcc_features(audio)
        print(f"      âœ“ MFCC extraÃ­dos ({mfcc_features.shape})")

        # Extraer embedding del hablante (independiente del texto)
        speaker_embedding = self._extract_speaker_embedding(mfcc_features)
        print(f"      âœ“ Embedding del hablante extraÃ­do ({speaker_embedding.shape[0]} dims)")

        prosodic_features = self._extract_prosodic_features(audio)
        print("      âœ“ CaracterÃ­sticas prosÃ³dicas extraÃ­das")

        return mfcc_features, speaker_embedding, prosodic_features
    
    def record_voice_sample(self, username):
        """
        Graba mÃºltiples muestras con DIFERENTES FRASES para crear perfil de voz
        Text-independent speaker verification
        """
        print(f"\n{'='*60}")
        print("   REGISTRO DE VOZ CON DESAFÃOS ALEATORIOS")
        print(f"{'='*60}")
        
        print(f"\nğŸ“‹ Nuevo Sistema de Seguridad:")
        print(f"   â€¢ Se grabarÃ¡n 5 muestras con FRASES DIFERENTES")
        print(f"   â€¢ Esto crea un perfil de tu voz Ãºnico")
        print(f"   â€¢ En cada inicio de sesiÃ³n dirÃ¡s una frase ALEATORIA")
        print(f"   â€¢ Previene ataques de replay (grabaciones)")
        
        print(f"\nğŸ’¡ Consejos:")
        print(f"   â€¢ Ambiente silencioso")
        print(f"   â€¢ Habla con naturalidad")
        print(f"   â€¢ Di cada frase claramente")
        
        input("\nPresiona ENTER para comenzar...")
        
        # Grabar 5 muestras con diferentes frases
        samples = []
        num_samples = 5
        
        for i in range(num_samples):
            print(f"\n{'â”€'*60}")
            print(f"   MUESTRA {i+1} de {num_samples}")
            print(f"{'â”€'*60}")
            
            # Generar desafÃ­o aleatorio
            challenge, display = ChallengeGenerator.generate_challenge(self.challenge_type)
            
            # Grabar
            audio = self._record_audio_with_challenge(challenge, display)

            # Procesar
            mfcc_features, speaker_embedding, prosodic_features = self._process_audio(audio)

            if mfcc_features is None:
                print("   âŒ Muestra invÃ¡lida")
                # Reintentar esta muestra
                i -= 1
                continue

            # Calcular calidad
            quality = (
                prosodic_features['rms_variance'] * 100 +
                prosodic_features['zcr_variance'] * 1000 +
                prosodic_features['pitch_variance']
            )

            print(f"\n   ğŸ“Š Calidad: {quality:.2f}")

            # Guardar muestra
            samples.append({
                'mfcc': mfcc_features,
                'embedding': speaker_embedding,
                'prosodic': prosodic_features,
                'quality': quality,
                'challenge': challenge
            })
            
            print(f"   âœ“ Muestra {i+1} guardada")
            
            if i < num_samples - 1:
                import time
                print("\n   Preparando siguiente frase...")
                time.sleep(2)
        
        if len(samples) == 0:
            print("\nâŒ No se pudo obtener muestras vÃ¡lidas")
            return None
        
        print(f"\n{'='*60}")
        print("âœ… PERFIL DE VOZ CREADO")
        print(f"   Muestras registradas: {len(samples)}")
        print(f"   Calidad promedio: {np.mean([s['quality'] for s in samples]):.2f}")
        print(f"{'='*60}")
        
        # Guardar todas las muestras
        voice_data = {
            'samples': samples,
            'num_samples': len(samples),
            'challenge_type': self.challenge_type,
            'version': 'challenge-response-v2'  # Nueva versiÃ³n con embeddings
        }

        return voice_data
    
    def verify_voice(self, username, stored_features):
        """
        Verifica identidad con FRASE ALEATORIA
        Text-independent verification usando embeddings del hablante
        """
        print(f"\n{'='*60}")
        print("   VERIFICACIÃ“N DE VOZ CON DESAFÃO ALEATORIO")
        print(f"{'='*60}")

        # Verificar formato de datos
        if not isinstance(stored_features, dict) or 'version' not in stored_features:
            print("\nâš ï¸  Datos en formato antiguo detectados")
            print("ğŸ’¡ Re-registra tu voz para usar el nuevo sistema con desafÃ­os")
            print("\n   Por ahora, no se puede verificar con desafÃ­o aleatorio")
            return False

        version = stored_features.get('version')
        if version not in ['challenge-response-v1', 'challenge-response-v2']:
            print("\nâš ï¸  VersiÃ³n de datos incompatible")
            return False

        # Generar desafÃ­o aleatorio
        challenge, display = ChallengeGenerator.generate_challenge(self.challenge_type)

        print(f"\nğŸ² DesafÃ­o Aleatorio Generado")
        print(f"   Este desafÃ­o es ÃšNICO para esta sesiÃ³n")
        print(f"   Previene ataques de replay\n")

        input("Presiona ENTER para comenzar la verificaciÃ³n...")

        # Grabar respuesta del usuario
        audio = self._record_audio_with_challenge(challenge, display)

        # Procesar
        mfcc_features, speaker_embedding, prosodic_features = self._process_audio(audio)

        if mfcc_features is None:
            print("âŒ Audio invÃ¡lido")
            return False

        # Verificar vivacidad (si estÃ¡ habilitado)
        if self.enable_liveness:
            print(f"\n{'â”€'*60}")
            print("   ğŸ” VERIFICANDO VIVACIDAD")
            print(f"{'â”€'*60}")

            is_live, confidence, messages = self._check_liveness(prosodic_features)

            for msg in messages:
                print(f"   {msg}")

            print(f"\n   Confianza: {confidence*100:.1f}%")

            if not is_live:
                print(f"\n   âš ï¸  Advertencia: DetecciÃ³n de vivacidad fallÃ³")
                print(f"   Continuando verificaciÃ³n...")

        # Comparar con muestras almacenadas
        print(f"\n{'â”€'*60}")
        print("   ğŸ” COMPARANDO CON PERFIL DE VOZ")
        print(f"{'â”€'*60}")

        stored_samples = stored_features['samples']
        similarities = []

        print(f"\n   Comparando con {len(stored_samples)} muestras...")

        # Usar embeddings si estÃ¡n disponibles (v2), sino usar MFCC (v1)
        use_embeddings = version == 'challenge-response-v2'

        for idx, sample in enumerate(stored_samples):
            if use_embeddings and 'embedding' in sample:
                # Comparar embeddings usando distancia coseno
                stored_embedding = sample['embedding']
                similarity = self._compare_embeddings(speaker_embedding, stored_embedding)
            else:
                # Fallback a comparaciÃ³n DTW de MFCC
                stored_mfcc = sample['mfcc']
                similarity, _ = self._compare_features_dtw(mfcc_features, stored_mfcc)

            similarities.append(similarity)
            print(f"   Muestra {idx+1}: {similarity*100:.2f}%")
        
        # Usar similitud promedio
        avg_similarity = np.mean(similarities)
        max_similarity = np.max(similarities)
        
        print(f"\n   ğŸ“Š Similitud promedio: {avg_similarity*100:.2f}%")
        print(f"   ğŸ“Š Similitud mÃ¡xima: {max_similarity*100:.2f}%")
        print(f"   ğŸ¯ Umbral requerido: {self.similarity_threshold*100:.2f}%")
        
        # DecisiÃ³n: usar promedio de las 3 mejores muestras
        top_3_similarities = sorted(similarities, reverse=True)[:3]
        final_similarity = np.mean(top_3_similarities)
        
        print(f"\n   ğŸ¯ Similitud final (top-3): {final_similarity*100:.2f}%")
        
        is_match = final_similarity >= self.similarity_threshold
        
        if is_match:
            print(f"\n{'='*60}")
            print("âœ… VERIFICACIÃ“N EXITOSA")
            print(f"   Identidad confirmada con {final_similarity*100:.2f}%")
            print(f"   Frase del desafÃ­o: '{challenge}'")
            print(f"{'='*60}")
        else:
            print(f"\n{'='*60}")
            print("âŒ VERIFICACIÃ“N FALLIDA")
            print(f"   Similitud insuficiente ({final_similarity*100:.2f}%)")
            print(f"{'='*60}")
        
        return is_match