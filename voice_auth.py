"""
Sistema de autenticaci√≥n de voz con CHALLENGE-RESPONSE
Genera frases aleatorias en cada verificaci√≥n para prevenir ataques de replay
"""

import numpy as np
import sounddevice as sd
from scipy.signal import butter, filtfilt
from scipy.spatial.distance import euclidean
from fastdtw import fastdtw
import librosa
from config import Config
from challenge_generator import ChallengeGenerator


class VoiceAuthChallenge:
    """
    Sistema de autenticaci√≥n de voz con desaf√≠os aleatorios
    Previene ataques de replay al requerir diferentes frases cada vez
    """
    
    def __init__(self):
        self.sample_rate = Config.VOICE_SAMPLE_RATE
        self.duration = Config.VOICE_DURATION
        self.similarity_threshold = getattr(Config, 'VOICE_SIMILARITY_THRESHOLD', 0.75)

        # Tipo de desaf√≠o por defecto
        self.challenge_type = getattr(Config, 'VOICE_CHALLENGE_TYPE', 'numeric')

        # Par√°metros MFCC
        self.n_mfcc = 13
        self.n_fft = 2048
        self.hop_length = 512

        # Par√°metros de detecci√≥n de vivacidad
        self.enable_liveness = getattr(Config, 'VOICE_ENABLE_LIVENESS', True)
        self.energy_variance_threshold = getattr(Config, 'VOICE_MIN_ENERGY_VARIANCE', 0.005)
        self.zcr_variance_threshold = getattr(Config, 'VOICE_MIN_ZCR_VARIANCE', 0.0005)
        self.pitch_variance_threshold = getattr(Config, 'VOICE_MIN_PITCH_VARIANCE', 2)

    def generate_challenge(self):
        """
        Genera una frase de desaf√≠o aleatoria
        Retorna solo el texto del desaf√≠o (para compatibilidad con Flask)
        """
        challenge_text, display_format = ChallengeGenerator.generate_challenge(self.challenge_type)
        return challenge_text
    
    def _apply_bandpass_filter(self, audio, lowcut=300, highcut=3400):
        """Aplica filtro pasabanda para voz humana"""
        nyquist = self.sample_rate / 2
        low = lowcut / nyquist
        high = highcut / nyquist
        b, a = butter(4, [low, high], btype='band')
        filtered = filtfilt(b, a, audio)
        return filtered
    
    def _normalize_audio(self, audio):
        """Normaliza el audio al rango [-1, 1]"""
        audio = audio.astype(np.float32)
        max_val = np.max(np.abs(audio))
        if max_val > 0:
            audio = audio / max_val
        return audio
    
    def _remove_silence(self, audio, threshold=0.01):
        """Elimina silencios del inicio y final con umbral m√°s permisivo"""
        window_size = int(self.sample_rate * 0.02)

        # Si el audio es muy corto, no procesar
        if len(audio) < window_size * 2:
            return audio

        energy = np.array([
            np.sum(audio[i:i+window_size]**2)
            for i in range(0, len(audio) - window_size, window_size)
        ])

        if np.max(energy) > 0:
            energy = energy / np.max(energy)

        voice_indices = np.where(energy > threshold)[0]

        # Si no hay suficientes √≠ndices, retornar audio completo
        if len(voice_indices) == 0:
            return audio

        # A√±adir margen para no cortar demasiado
        start_idx = max(0, (voice_indices[0] - 1) * window_size)
        end_idx = min(len(audio), (voice_indices[-1] + 2) * window_size)

        trimmed_audio = audio[start_idx:end_idx]

        # Asegurar que el audio resultante no sea demasiado corto
        if len(trimmed_audio) < self.sample_rate * 0.5:  # Al menos 0.5 segundos
            return audio

        return trimmed_audio
    
    def _extract_mfcc_features(self, audio):
        """Extrae caracter√≠sticas MFCC + deltas y calcula estad√≠sticas"""
        mfcc = librosa.feature.mfcc(
            y=audio,
            sr=self.sample_rate,
            n_mfcc=self.n_mfcc,
            n_fft=self.n_fft,
            hop_length=self.hop_length
        )

        mfcc_delta = librosa.feature.delta(mfcc)
        mfcc_delta2 = librosa.feature.delta(mfcc, order=2)

        features = np.vstack([mfcc, mfcc_delta, mfcc_delta2])

        return features

    def _extract_speaker_embedding(self, mfcc_features):
        """
        Extrae un vector de embedding del hablante independiente del texto
        Calcula estad√≠sticas sobre los MFCC que caracterizan la voz, no el contenido

        IMPORTANTE: Solo los primeros 13 coeficientes MFCC representan la voz del hablante.
        Los deltas (14-26) y delta-deltas (27-39) capturan informaci√≥n temporal/din√°mica
        que var√≠a mucho incluso para la misma persona diciendo cosas diferentes.
        """
        # Separar MFCC base (0-12) de deltas y delta-deltas
        mfcc_base = mfcc_features[:13, :]  # Solo los primeros 13 coeficientes

        # Calcular estad√≠sticas SOLO sobre los MFCC base para mayor discriminaci√≥n
        mean = np.mean(mfcc_base, axis=1)
        std = np.std(mfcc_base, axis=1)

        # Percentiles para capturar la distribuci√≥n de las caracter√≠sticas del hablante
        percentile_10 = np.percentile(mfcc_base, 10, axis=1)
        percentile_25 = np.percentile(mfcc_base, 25, axis=1)
        percentile_50 = np.percentile(mfcc_base, 50, axis=1)
        percentile_75 = np.percentile(mfcc_base, 75, axis=1)
        percentile_90 = np.percentile(mfcc_base, 90, axis=1)

        # Rango intercuart√≠lico - captura variabilidad
        iqr = percentile_75 - percentile_25

        # Rango completo
        mfcc_min = np.min(mfcc_base, axis=1)
        mfcc_max = np.max(mfcc_base, axis=1)
        mfcc_range = mfcc_max - mfcc_min

        # Skewness aproximado
        median = percentile_50
        skewness = mean - median

        # Concatenar todas las estad√≠sticas
        embedding = np.concatenate([
            mean,           # 13 valores - caracter√≠sticas centrales de la voz
            std,            # 13 valores - variabilidad
            percentile_10,  # 13 valores
            percentile_25,  # 13 valores
            percentile_50,  # 13 valores (mediana)
            percentile_75,  # 13 valores
            percentile_90,  # 13 valores
            iqr,            # 13 valores - rango intercuart√≠lico
            mfcc_range,     # 13 valores - rango total
            skewness        # 13 valores - asimetr√≠a
        ])

        # Vector de 130 dimensiones (13 * 10)
        return embedding
    
    def _extract_prosodic_features(self, audio):
        """Extrae caracter√≠sticas pros√≥dicas"""
        rms = librosa.feature.rms(
            y=audio,
            frame_length=self.n_fft,
            hop_length=self.hop_length
        )[0]
        
        zcr = librosa.feature.zero_crossing_rate(
            audio,
            frame_length=self.n_fft,
            hop_length=self.hop_length
        )[0]
        
        try:
            pitches, magnitudes = librosa.piptrack(
                y=audio,
                sr=self.sample_rate,
                n_fft=self.n_fft,
                hop_length=self.hop_length
            )
            pitch = []
            for t in range(pitches.shape[1]):
                index = magnitudes[:, t].argmax()
                pitch.append(pitches[index, t])
            pitch = np.array(pitch)
        except:
            pitch = np.zeros(len(rms))
        
        return {
            'rms': rms,
            'zcr': zcr,
            'pitch': pitch,
            'rms_variance': np.var(rms),
            'zcr_variance': np.var(zcr),
            'pitch_variance': np.var(pitch[pitch > 0]) if np.any(pitch > 0) else 0
        }
    
    def _check_liveness(self, prosodic_features):
        """Verifica vivacidad del audio"""
        checks = []
        messages = []
        
        energy_var = prosodic_features['rms_variance']
        energy_check = energy_var > self.energy_variance_threshold
        checks.append(energy_check)
        
        if energy_check:
            messages.append(f"‚úì Energ√≠a natural ({energy_var:.6f})")
        else:
            messages.append(f"‚ö†Ô∏è  Energ√≠a baja ({energy_var:.6f})")
        
        zcr_var = prosodic_features['zcr_variance']
        zcr_check = zcr_var > self.zcr_variance_threshold
        checks.append(zcr_check)
        
        if zcr_check:
            messages.append(f"‚úì ZCR natural ({zcr_var:.6f})")
        else:
            messages.append(f"‚ö†Ô∏è  ZCR bajo ({zcr_var:.6f})")
        
        pitch_var = prosodic_features['pitch_variance']
        pitch_check = pitch_var > self.pitch_variance_threshold
        checks.append(pitch_check)
        
        if pitch_check:
            messages.append(f"‚úì Pitch natural ({pitch_var:.2f})")
        else:
            messages.append(f"‚ö†Ô∏è  Pitch bajo ({pitch_var:.2f})")
        
        confidence = sum(checks) / len(checks)
        is_live = confidence >= 0.34
        
        if sum(checks) == 0:
            is_live = False
            messages.append("‚ö†Ô∏è  Audio puede ser sint√©tico")
        
        return is_live, confidence, messages
    
    def _compare_embeddings(self, embedding1, embedding2):
        """
        Compara embeddings de hablante usando distancia euclidiana normalizada
        con funci√≥n de decaimiento ajustada para caracter√≠sticas de voz
        """
        try:
            emb1 = np.array(embedding1)
            emb2 = np.array(embedding2)

            # Calcular distancia euclidiana
            euclidean_distance = np.linalg.norm(emb1 - emb2)

            # Normalizar la distancia dividi√©ndola por sqrt(dimensiones)
            # Esto da una "distancia promedio por dimensi√≥n"
            normalized_distance = euclidean_distance / np.sqrt(len(emb1))

            # Convertir distancia a similitud [0, 1]
            # Con distancia euclidiana, los valores son m√°s altos que con coseno
            # decay_factor=22: misma persona ~55-65%, diferente persona ~25-35%
            decay_factor = 22.0
            similarity = np.exp(-normalized_distance / decay_factor)

            return similarity

        except Exception as e:
            print(f"\n   ‚ö†Ô∏è  Error en comparaci√≥n de embeddings: {e}")
            return 0.0

    def _compare_features_dtw(self, features1, features2):
        """Compara caracter√≠sticas usando DTW"""
        try:
            feat1 = np.array(features1)
            feat2 = np.array(features2)

            if feat1.ndim == 1:
                feat1 = feat1.reshape(-1, 1)
            if feat2.ndim == 1:
                feat2 = feat2.reshape(-1, 1)

            # Transponer para que cada fila sea un frame temporal
            feat1 = feat1.T
            feat2 = feat2.T

            # Calcular DTW
            distance, path = fastdtw(feat1, feat2, dist=euclidean)

            # Normalizar por el n√∫mero de frames y dimensiones
            avg_length = (len(feat1) + len(feat2)) / 2
            n_features = feat1.shape[1]

            # Normalizaci√≥n mejorada para MFCC
            normalized_distance = distance / (avg_length * np.sqrt(n_features))

            # Convertir a similitud (0-1)
            similarity = 1 / (1 + normalized_distance)

            return similarity, normalized_distance

        except Exception as e:
            print(f"\n   ‚ö†Ô∏è  Error en comparaci√≥n DTW: {e}")
            return 0.0, float('inf')
    
    def _record_audio_with_challenge(self, challenge_text, display_format):
        """Graba audio mostrando el desaf√≠o al usuario"""
        print(f"\n{display_format}")
        print(f"\n   ‚ïî{'‚ïê'*56}‚ïó")
        print(f"   ‚ïë  {challenge_text:^52}  ‚ïë")
        print(f"   ‚ïö{'‚ïê'*56}‚ïù")
        
        print(f"\n‚è±Ô∏è  Duraci√≥n: {self.duration} segundos")
        print("\nüî¥ Grabando en 3...")
        
        import time
        for i in range(3, 0, -1):
            print(f"   {i}...")
            time.sleep(1)
        
        print("üéôÔ∏è  ¬°HABLA AHORA!\n")
        
        recording = sd.rec(
            int(self.duration * self.sample_rate),
            samplerate=self.sample_rate,
            channels=1,
            dtype=np.float32
        )
        sd.wait()
        
        print("‚úÖ Grabaci√≥n completada\n")
        
        return recording.flatten()
    
    def _process_audio(self, audio):
        """Pipeline completo de procesamiento"""
        print("   üîÑ Procesando audio...")

        audio = self._normalize_audio(audio)
        print("      ‚úì Normalizado")

        audio = self._apply_bandpass_filter(audio)
        print("      ‚úì Filtrado")

        audio = self._remove_silence(audio)
        print(f"      ‚úì Silencios eliminados (duraci√≥n: {len(audio)/self.sample_rate:.2f}s)")

        # Reducir umbral m√≠nimo a 0.5 segundos (antes era 1.0)
        min_length = self.sample_rate * 0.5
        if len(audio) < min_length:
            print(f"      ‚ö†Ô∏è  Audio muy corto: {len(audio)/self.sample_rate:.2f}s (m√≠nimo: 0.5s)")
            return None, None, None

        mfcc_features = self._extract_mfcc_features(audio)
        print(f"      ‚úì MFCC extra√≠dos ({mfcc_features.shape})")

        # Extraer embedding del hablante (independiente del texto)
        speaker_embedding = self._extract_speaker_embedding(mfcc_features)
        print(f"      ‚úì Embedding del hablante extra√≠do ({speaker_embedding.shape[0]} dims)")

        prosodic_features = self._extract_prosodic_features(audio)
        print("      ‚úì Caracter√≠sticas pros√≥dicas extra√≠das")

        return mfcc_features, speaker_embedding, prosodic_features
    
    def record_voice_sample(self, username):
        """
        Graba m√∫ltiples muestras con DIFERENTES FRASES para crear perfil de voz
        Text-independent speaker verification
        """
        print(f"\n{'='*60}")
        print("   REGISTRO DE VOZ CON DESAF√çOS ALEATORIOS")
        print(f"{'='*60}")
        
        print(f"\nüìã Nuevo Sistema de Seguridad:")
        print(f"   ‚Ä¢ Se grabar√°n 5 muestras con FRASES DIFERENTES")
        print(f"   ‚Ä¢ Esto crea un perfil de tu voz √∫nico")
        print(f"   ‚Ä¢ En cada inicio de sesi√≥n dir√°s una frase ALEATORIA")
        print(f"   ‚Ä¢ Previene ataques de replay (grabaciones)")
        
        print(f"\nüí° Consejos:")
        print(f"   ‚Ä¢ Ambiente silencioso")
        print(f"   ‚Ä¢ Habla con naturalidad")
        print(f"   ‚Ä¢ Di cada frase claramente")
        
        input("\nPresiona ENTER para comenzar...")
        
        # Grabar 5 muestras con diferentes frases
        samples = []
        num_samples = 5
        
        for i in range(num_samples):
            print(f"\n{'‚îÄ'*60}")
            print(f"   MUESTRA {i+1} de {num_samples}")
            print(f"{'‚îÄ'*60}")
            
            # Generar desaf√≠o aleatorio
            challenge, display = ChallengeGenerator.generate_challenge(self.challenge_type)
            
            # Grabar
            audio = self._record_audio_with_challenge(challenge, display)

            # Procesar
            mfcc_features, speaker_embedding, prosodic_features = self._process_audio(audio)

            if mfcc_features is None:
                print("   ‚ùå Muestra inv√°lida")
                # Reintentar esta muestra
                i -= 1
                continue

            # Calcular calidad
            quality = (
                prosodic_features['rms_variance'] * 100 +
                prosodic_features['zcr_variance'] * 1000 +
                prosodic_features['pitch_variance']
            )

            print(f"\n   üìä Calidad: {quality:.2f}")

            # Guardar muestra
            samples.append({
                'mfcc': mfcc_features,
                'embedding': speaker_embedding,
                'prosodic': prosodic_features,
                'quality': quality,
                'challenge': challenge
            })
            
            print(f"   ‚úì Muestra {i+1} guardada")
            
            if i < num_samples - 1:
                import time
                print("\n   Preparando siguiente frase...")
                time.sleep(2)
        
        if len(samples) == 0:
            print("\n‚ùå No se pudo obtener muestras v√°lidas")
            return None
        
        print(f"\n{'='*60}")
        print("‚úÖ PERFIL DE VOZ CREADO")
        print(f"   Muestras registradas: {len(samples)}")
        print(f"   Calidad promedio: {np.mean([s['quality'] for s in samples]):.2f}")
        print(f"{'='*60}")
        
        # Guardar todas las muestras
        voice_data = {
            'samples': samples,
            'num_samples': len(samples),
            'challenge_type': self.challenge_type,
            'version': 'challenge-response-v2'  # Nueva versi√≥n con embeddings
        }

        return voice_data
    
    def verify_voice(self, username, stored_features):
        """
        Verifica identidad con FRASE ALEATORIA
        Text-independent verification usando embeddings del hablante
        """
        print(f"\n{'='*60}")
        print("   VERIFICACI√ìN DE VOZ CON DESAF√çO ALEATORIO")
        print(f"{'='*60}")

        # Verificar formato de datos
        if not isinstance(stored_features, dict) or 'version' not in stored_features:
            print("\n‚ö†Ô∏è  Datos en formato antiguo detectados")
            print("üí° Re-registra tu voz para usar el nuevo sistema con desaf√≠os")
            print("\n   Por ahora, no se puede verificar con desaf√≠o aleatorio")
            return False

        version = stored_features.get('version')
        if version not in ['challenge-response-v1', 'challenge-response-v2']:
            print("\n‚ö†Ô∏è  Versi√≥n de datos incompatible")
            return False

        # Generar desaf√≠o aleatorio
        challenge, display = ChallengeGenerator.generate_challenge(self.challenge_type)

        print(f"\nüé≤ Desaf√≠o Aleatorio Generado")
        print(f"   Este desaf√≠o es √öNICO para esta sesi√≥n")
        print(f"   Previene ataques de replay\n")

        input("Presiona ENTER para comenzar la verificaci√≥n...")

        # Grabar respuesta del usuario
        audio = self._record_audio_with_challenge(challenge, display)

        # Procesar
        mfcc_features, speaker_embedding, prosodic_features = self._process_audio(audio)

        if mfcc_features is None:
            print("‚ùå Audio inv√°lido")
            return False

        # Verificar vivacidad (si est√° habilitado)
        if self.enable_liveness:
            print(f"\n{'‚îÄ'*60}")
            print("   üîç VERIFICANDO VIVACIDAD")
            print(f"{'‚îÄ'*60}")

            is_live, confidence, messages = self._check_liveness(prosodic_features)

            for msg in messages:
                print(f"   {msg}")

            print(f"\n   Confianza: {confidence*100:.1f}%")

            if not is_live:
                print(f"\n   ‚ö†Ô∏è  Advertencia: Detecci√≥n de vivacidad fall√≥")
                print(f"   Continuando verificaci√≥n...")

        # Comparar con muestras almacenadas
        print(f"\n{'‚îÄ'*60}")
        print("   üîç COMPARANDO CON PERFIL DE VOZ")
        print(f"{'‚îÄ'*60}")

        stored_samples = stored_features['samples']
        similarities = []

        print(f"\n   Comparando con {len(stored_samples)} muestras...")

        # Usar embeddings si est√°n disponibles (v2), sino usar MFCC (v1)
        use_embeddings = version == 'challenge-response-v2'

        for idx, sample in enumerate(stored_samples):
            if use_embeddings and 'embedding' in sample:
                # Comparar embeddings usando distancia coseno
                stored_embedding = sample['embedding']
                similarity = self._compare_embeddings(speaker_embedding, stored_embedding)
            else:
                # Fallback a comparaci√≥n DTW de MFCC
                stored_mfcc = sample['mfcc']
                similarity, _ = self._compare_features_dtw(mfcc_features, stored_mfcc)

            similarities.append(similarity)
            print(f"   Muestra {idx+1}: {similarity*100:.2f}%")
        
        # Usar similitud promedio
        avg_similarity = np.mean(similarities)
        max_similarity = np.max(similarities)
        
        print(f"\n   üìä Similitud promedio: {avg_similarity*100:.2f}%")
        print(f"   üìä Similitud m√°xima: {max_similarity*100:.2f}%")
        print(f"   üéØ Umbral requerido: {self.similarity_threshold*100:.2f}%")
        
        # Decisi√≥n: usar promedio de las 3 mejores muestras
        top_3_similarities = sorted(similarities, reverse=True)[:3]
        final_similarity = np.mean(top_3_similarities)
        
        print(f"\n   üéØ Similitud final (top-3): {final_similarity*100:.2f}%")
        
        is_match = final_similarity >= self.similarity_threshold
        
        if is_match:
            print(f"\n{'='*60}")
            print("‚úÖ VERIFICACI√ìN EXITOSA")
            print(f"   Identidad confirmada con {final_similarity*100:.2f}%")
            print(f"   Frase del desaf√≠o: '{challenge}'")
            print(f"{'='*60}")
        else:
            print(f"\n{'='*60}")
            print("‚ùå VERIFICACI√ìN FALLIDA")
            print(f"   Similitud insuficiente ({final_similarity*100:.2f}%)")
            print(f"{'='*60}")
        
        return is_match